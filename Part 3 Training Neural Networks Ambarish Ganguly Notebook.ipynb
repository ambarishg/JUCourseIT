{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our data\n",
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass, get our logits\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "logits = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2996, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8253, -0.3574],\n",
      "        [ 0.5915,  1.3430]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6810, 0.1277],\n",
      "        [0.3499, 1.8036]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x000000E903223A90>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7406, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8253, -0.3574],\n",
       "        [ 0.5915,  1.3430]], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4126, -0.1787],\n",
      "        [ 0.2958,  0.6715]])\n",
      "tensor([[ 0.4126, -0.1787],\n",
      "        [ 0.2958,  0.6715]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Autograd together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0020, -0.0020, -0.0020,  ..., -0.0020, -0.0020, -0.0020],\n",
      "        [-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0030, -0.0030, -0.0030,  ..., -0.0030, -0.0030, -0.0030],\n",
      "        [ 0.0010,  0.0010,  0.0010,  ...,  0.0010,  0.0010,  0.0010],\n",
      "        [ 0.0029,  0.0029,  0.0029,  ...,  0.0029,  0.0029,  0.0029]])\n"
     ]
    }
   ],
   "source": [
    "print(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.9498089168117498\n",
      "Training loss: 0.8835471659453947\n",
      "Training loss: 0.5336451658816226\n",
      "Training loss: 0.43445018308757466\n",
      "Training loss: 0.3895100897340886\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8098e-03, 1.7224e-05, 3.4715e-04, 9.4837e-01, 3.2507e-06, 4.7019e-02,\n",
       "         1.3543e-05, 7.4434e-06, 2.3978e-03, 9.7567e-06]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
