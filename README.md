# Introduction to Deep Learning 

**Venue : Jadavpur University Information Technology Department** 
 
explaining the concepts from Percepton , Activation Functions , 
Forward Propagation , Backward propagation , Gradient Descent , 
Stochastic Gradient Descent , Mini Batch Gradient Descent , 
Dropout , Transfer Learning and also CNN Concepts such as Filters , Pooling .    

The theory was interspersed with generous coding demos. 
The attendees also did an extensive hands-on for more than 2 hours.    

My thanks to all the Professors and all the attendees of the session. Special thanks to  the material that I had used in the lecture from
-  Alexander Amini Intro to Deep Learning ( https://lnkd.in/fWgYM8t )
- Brandon Rohrer slides on Convolution Neural Networks (https://lnkd.in/fxvCsTE )
- Intro to Deep Learning using PyTorch -Udacity Free Course
- Kaggle for using the hands-on

The Slides have been kept in the repository for ready reference.      
The Notebooks have been kept for ready reference.           
The Dogs and Cats dataset used in Part 8 Notebook can be obtained from the Kaggle competition 
(https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition)      
The Digits dataset in the CNN Notebook can be obtained from the Kaggle competition  
( https://www.kaggle.com/c/digit-recognizer )
